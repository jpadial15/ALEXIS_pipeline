{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "sys.path.insert(1, '..')\n",
    "sys.path.insert(2, '../modules/')\n",
    "\n",
    "from glob import glob\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# from ruffus.task import fill_queue_with_job_parameters\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import query_the_data\n",
    "\n",
    "import aia_module\n",
    "\n",
    "import dataconfig\n",
    "import convert_datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import clean_img_data_02\n",
    "import sxi_module\n",
    "import helio_reg_exp_module\n",
    "\n",
    "import coordinate_conversion_module\n",
    "\n",
    "import convex_fits_and_filtering_module\n",
    "\n",
    "import associate_HARP_to_flare_region_module\n",
    "\n",
    "import associate_candidates_to_flare_meta_module\n",
    "\n",
    "import check_data_qual_module_02\n",
    "\n",
    "import LASSO_metrics_module\n",
    "\n",
    "from skimage.transform import rotate\n",
    "import sunpy.map\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import ALEXIS_02_plotting_module as plot_mod\n",
    "\n",
    "import ALEXIS_02_define_goes_class_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiles = glob(f'{dataconfig.DATA_DIR_FLARE_CANDIDATES}/*.working/ALEXIS_flares_w_harp_goes_class_and_known_flare_meta.pickle')\n",
    "len(infiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = infiles[0]\n",
    "\n",
    "these_flares = pd.DataFrame(pickle.load(open(infile, 'rb')))\n",
    "\n",
    "working_dir = these_flares.iloc[0].working_dir\n",
    "\n",
    "good_quality_data_df = pickle.load(open(f'{working_dir}/initialize_with_these_files_df.pickle', 'rb'))\n",
    "\n",
    "mask_properties = these_flares.iloc[0]\n",
    "\n",
    "this_wl, this_inst, this_tel = mask_properties.img_wavelength, mask_properties.img_instrument, mask_properties.img_telescope\n",
    "\n",
    "these_images = good_quality_data_df[(good_quality_data_df.wavelength == this_wl) & (good_quality_data_df.telescope == this_tel) & (good_quality_data_df.instrument == this_inst) & (good_quality_data_df.QUALITY == 0)]\n",
    "\n",
    "these_images['catalog_file'] = [infile for _ in these_images.file_path]\n",
    "\n",
    "# load and save xrs data\n",
    "\n",
    "xrs_b_data = ALEXIS_02_define_goes_class_module.find_xrs_data(these_flares)\n",
    "\n",
    "xrs_real_data_file_path = os.path.join(working_dir, 'real_xrs_data.pickle')\n",
    "\n",
    "pickle.dump(xrs_b_data, open(xrs_real_data_file_path, 'wb'))\n",
    "\n",
    "these_images['real_xrs_data_path'] = [xrs_real_data_file_path for _ in these_images.file_path]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_zoom_ins_for_movie(axis, this_img_df, catalog_df, tracking_number, cluster_label):\n",
    "    \n",
    "    df = catalog_df.iloc[0]\n",
    "    \n",
    "    init_file = f'{df.working_dir}/initialize_with_these_files_df.pickle'\n",
    "\n",
    "    init_files = pickle.load(open(init_file, 'rb'))\n",
    "\n",
    "    masked_img = init_files[(init_files.wavelength == df.img_wavelength ) & (init_files.telescope == df.img_telescope ) ]\n",
    "\n",
    "    instrument = df.img_instrument\n",
    "    \n",
    "    all_imgs_max = masked_img.img_data_max.max()\n",
    "\n",
    "    if instrument == 'AIA':\n",
    "\n",
    "        img_dict, data,header = clean_img_data_02.clean_aia_data(this_img_df.to_dict('records')[0])\n",
    "        \n",
    "        data_map = sunpy.map.Map(data, header)\n",
    "        \n",
    "        r_sun = header['R_SUN']\n",
    "\n",
    "    if instrument == 'SXI':\n",
    "\n",
    "        img_dict, raw_data, header = clean_img_data_02.clean_sxi_data(this_img_df.to_dict('records')[0])\n",
    "\n",
    "        rotated_data = rotate(np.float32(raw_data), -1*header['CROTA1'])\n",
    "\n",
    "        data = rotated_data/masked_img.to_dict('records')[0]['exp_time']\n",
    "        \n",
    "        data_map = sunpy.map.Map(data, header)\n",
    "        \n",
    "        r_sun = header['RSUN']\n",
    "        \n",
    "    ##### \n",
    "    cmap_dict = {94: 'sdoaia94', 131: 'sdoaia131', 171: 'sdoaia171', 193:'sdoaia193', 211: 'sdoaia211', 335:'sdoaia335', 304: 'sdoaia304', 'TM': 'sohoeit304', 'PTHK': 'sohoeit195', 'PTHNA': 'sohoeit284'}\n",
    "    cluster_color_dict = {0: 'blue', 1: 'orange', 2: 'green', 3: 'purple'}\n",
    "    id_team_dict = {'SolarSoft': 'red', 'SWPC': 'yellow'}\n",
    "    \n",
    "    subset_range = np.array(df.integration_pixels_bbox[tracking_number])\n",
    "    \n",
    "#     print(integration_pix_bbox)\n",
    "    \n",
    "#     # Extract the subset of data within the range\n",
    "#     x_min, y_min = subset_range.min(axis=0)\n",
    "#     x_max, y_max = subset_range.max(axis=0)\n",
    "#     subset_data = data[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "    # axis.imshow(subset_data/data.max(), origin = 'lower',edgecolor='red')\n",
    "\n",
    "    # Plot the full data\n",
    "    axis.imshow(data/all_imgs_max, origin = 'lower', cmap = cmap_dict[df.img_wavelength])\n",
    "\n",
    "    # Set the x and y limits to the subset range\n",
    "    axis.set_xlim(subset_range[:, 0].min(), subset_range[:, 0].max())\n",
    "    axis.set_ylim(subset_range[:, 1].min(), subset_range[:, 1].max())\n",
    "    \n",
    "    # Create a rectangle patch for the border\n",
    "    x_min, y_min = subset_range.min(axis=0)\n",
    "    x_max, y_max = subset_range.max(axis=0)\n",
    "    rect = patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=10, edgecolor=cluster_color_dict[cluster_label], facecolor='none')\n",
    "\n",
    "    # Add the rectangle patch to the axes\n",
    "    axis.add_patch(rect)\n",
    "\n",
    "    # Remove ticks on x and y axes\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    \n",
    "    # plot known flares location\n",
    "\n",
    "    \n",
    "    flare_meta_file = f'{df.working_dir}/known_flares.pickle'\n",
    "\n",
    "    flare_meta_df = pickle.load(open(flare_meta_file, 'rb'))\n",
    "\n",
    "    id_team_dict = {'SolarSoft': 'yellow', 'SWPC': 'red'}\n",
    "\n",
    "    for id_team, id_group in flare_meta_df.groupby('id_team'):\n",
    "    \n",
    "        for number, iterrow in enumerate(id_group.sort_values(by='peak_time').iterrows()):\n",
    "\n",
    "            _, row = iterrow[0], iterrow[1]\n",
    "            \n",
    "            x_hpc, y_hpc = row['hpc_x'], row['hpc_y']\n",
    "            \n",
    "            x_pix = (x_hpc/header['CDELT1']) + header['CRPIX1'] \n",
    "\n",
    "            y_pix = (y_hpc/header['CDELT1']) + header['CRPIX2']\n",
    "            \n",
    "            id_team = row['id_team']\n",
    "\n",
    "            team_dict = {'SolarSoft': 'Lockheed Martin', 'SWPC': 'NOAA'}\n",
    "\n",
    "            this_time = convert_datetime.astropytime_to_pythondatetime(row.peak_time).strftime('%H:%M')\n",
    "\n",
    "            marker_dict = {0: 'v', 1:'^', 2:'<'}\n",
    "            \n",
    "            axis.scatter(x_pix, y_pix, s = 200, color = id_team_dict[row['id_team']], label = f'{team_dict[id_team]} @ {this_time} w/ {row.goes_class}', marker = marker_dict[number])\n",
    "   \n",
    "    # for _, row in flare_meta_df.iterrows():\n",
    "        \n",
    "    #     x_hpc, y_hpc = row['hpc_x'], row['hpc_y']\n",
    "        \n",
    "    #     x_pix = (x_hpc/header['CDELT1']) + header['CRPIX1'] \n",
    "\n",
    "    #     y_pix = (y_hpc/header['CDELT1']) + header['CRPIX2']\n",
    "        \n",
    "    #     axis.scatter(x_pix, y_pix, s = 500, color = id_team_dict[row['id_team']], alpha = 0.3)\n",
    "        \n",
    "        \n",
    "    \n",
    "    #create span of AR dataframe\n",
    "    span_of_harps_df = pickle.load(open(df.span_of_harp_file_path, 'rb'))\n",
    "    \n",
    "#     print(span_of_harps_df)\n",
    "\n",
    "    for HARPNUM, harp_group in span_of_harps_df.groupby('HARPNUM'):\n",
    "        \n",
    "        pix_array = np.array(harp_group.span_pix_bbox.iloc[0]).T\n",
    "        \n",
    "#         print(pix_array[0,:])\n",
    "        axis.plot(pix_array[0,:],pix_array[1,:], color = 'grey', linewidth = 3)\n",
    "    \n",
    "    def make_circle_xy(radius, center):\n",
    "        theta = np.linspace( 0 , 2 * np.pi , 150 )\n",
    "\n",
    "        radius = radius\n",
    "\n",
    "        r_1,r_2 = center[0], center[1]\n",
    "\n",
    "        a = r_1 + radius * np.cos( theta )\n",
    "        b = r_2 + radius * np.sin( theta )\n",
    "\n",
    "        return(a,b)\n",
    "    \n",
    "    circle = make_circle_xy(r_sun, (header['CRPIX1'],header['CRPIX1']))\n",
    "\n",
    "    axis.plot(circle[0],circle[1], linewidth = 4, color = 'coral')\n",
    "    \n",
    "    # plot alexis pix coord pair\n",
    "    \n",
    "    \n",
    "    alexis_pix_tuple = df.pix_coord_tuple\n",
    "    \n",
    "    this_pix_coords = alexis_pix_tuple[tracking_number]\n",
    "    \n",
    "    axis.scatter(this_pix_coords[0], this_pix_coords[1], s = 500, color = cluster_color_dict[cluster_label], alpha = 0.3 )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_fit_image_movie(axis, this_img_df, catalog_df):\n",
    "    \n",
    "    df = catalog_df.iloc[0]\n",
    "    \n",
    "    init_file = f'{df.working_dir}/initialize_with_these_files_df.pickle'\n",
    "\n",
    "    init_files = pickle.load(open(init_file, 'rb'))\n",
    "\n",
    "    masked_img = init_files[(init_files.wavelength == df.img_wavelength ) & (init_files.telescope == df.img_telescope ) ]\n",
    "\n",
    "    instrument = df.img_instrument\n",
    "    \n",
    "    all_imgs_max = masked_img.img_data_max.max()\n",
    "\n",
    "    if instrument == 'AIA':\n",
    "\n",
    "        img_dict, data,header = clean_img_data_02.clean_aia_data(this_img_df.to_dict('records')[0])\n",
    "        \n",
    "        data_map = sunpy.map.Map(data, header)\n",
    "        \n",
    "        r_sun = header['R_SUN']\n",
    "\n",
    "    if instrument == 'SXI':\n",
    "\n",
    "        img_dict, raw_data, header = clean_img_data_02.clean_sxi_data(this_img_df.to_dict('records')[0])\n",
    "\n",
    "        rotated_data = rotate(np.float32(raw_data), -1*header['CROTA1'])\n",
    "\n",
    "        data = rotated_data/masked_img.to_dict('records')[0]['exp_time']\n",
    "        \n",
    "        data_map = sunpy.map.Map(data, header)\n",
    "        \n",
    "        r_sun = header['RSUN']\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # print(best_2.img_wavelength)\n",
    "    cmap_dict = {94: 'sdoaia94', 131: 'sdoaia131', 171: 'sdoaia171', 193:'sdoaia193', 211: 'sdoaia211', 335:'sdoaia335', 304: 'sdoaia304', 'TM': 'sohoeit304', 'PTHK': 'sohoeit195', 'PTHNA': 'sohoeit284'}\n",
    "    cluster_color_dict = {0: 'blue', 1: 'orange', 2: 'green', 3: 'purple'}\n",
    "\n",
    "    axis.imshow(data/all_imgs_max, origin = 'lower', cmap = cmap_dict[df.img_wavelength], vmin =np.max(data/all_imgs_max)*.50,  vmax = np.max(data/all_imgs_max)*.90)\n",
    "#     axis.imshow(data, origin = 'lower', cmap = cmap_dict[df.img_wavelength],  vmax = np.max(data)*.10)\n",
    "\n",
    "    # plot the integration areas\n",
    "    \n",
    "    for tracking_number, cluster_label in enumerate(df.gridsearch_clusters):\n",
    "        \n",
    "        integration_pix_bbox = np.array(df.integration_pixels_bbox[tracking_number])\n",
    "\n",
    "        axis.plot(integration_pix_bbox[:,0], integration_pix_bbox[:,1], linewidth = 3, color = cluster_color_dict[cluster_label], label = f'ALEXIS cluster {cluster_label} integration')\n",
    "\n",
    "        pix_coords = df.pix_coord_tuple\n",
    "\n",
    "#         for pix_pair in pix_coords:\n",
    "\n",
    "        axis.scatter(pix_coords[tracking_number][0], pix_coords[tracking_number][1], color = cluster_color_dict[cluster_label])\n",
    "    \n",
    "\n",
    "    #create span of AR dataframe\n",
    "    span_of_harps_df = pickle.load(open(df.span_of_harp_file_path, 'rb'))\n",
    "    \n",
    "#     print(span_of_harps_df)\n",
    "\n",
    "    for HARPNUM, harp_group in span_of_harps_df.groupby('HARPNUM'):\n",
    "        \n",
    "        pix_array = np.array(harp_group.span_pix_bbox.iloc[0]).T\n",
    "        \n",
    "#         print(pix_array[0,:])\n",
    "        axis.plot(pix_array[0,:],pix_array[1,:], color = 'grey', linewidth = 3)\n",
    "    # annotate harp\n",
    "    \n",
    "    if df.img_instrument == 'AIA':\n",
    "\n",
    "        harp_annotate_x, harp_annotate_y = 1000, 3900\n",
    "    else: \n",
    "        harp_annotate_x, harp_annotate_y = 150, 425   \n",
    "        \n",
    "    \n",
    "    axis.annotate('HARPS boundary box', (harp_annotate_x, harp_annotate_y), color = 'grey', ha='center', va = 'bottom', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    \n",
    "    def make_circle_xy(radius, center):\n",
    "        \n",
    "        \n",
    "        theta = np.linspace( 0 , 2 * np.pi , 150 )\n",
    "\n",
    "        radius = radius\n",
    "\n",
    "        r_1,r_2 = center[0], center[1]\n",
    "\n",
    "        a = r_1 + radius * np.cos( theta )\n",
    "        b = r_2 + radius * np.sin( theta )\n",
    "\n",
    "        return(a,b)\n",
    "    \n",
    "    circle = make_circle_xy(r_sun, (header['CRPIX1'],header['CRPIX1']))\n",
    "\n",
    "    axis.plot(circle[0],circle[1], linewidth = 4, color = 'coral')\n",
    "    \n",
    "    \n",
    "    # plot known flares location\n",
    "\n",
    "    flare_meta_file = f'{df.working_dir}/known_flares.pickle'\n",
    "\n",
    "    flare_meta_df = pickle.load(open(flare_meta_file, 'rb'))\n",
    "\n",
    "    id_team_dict = {'SolarSoft': 'yellow', 'SWPC': 'red'}\n",
    "\n",
    "    for id_team, id_group in flare_meta_df.groupby('id_team'):\n",
    "    \n",
    "        for number, iterrow in enumerate(id_group.sort_values(by='peak_time').iterrows()):\n",
    "\n",
    "            _, row = iterrow[0], iterrow[1]\n",
    "            \n",
    "            x_hpc, y_hpc = row['hpc_x'], row['hpc_y']\n",
    "            \n",
    "            x_pix = (x_hpc/header['CDELT1']) + header['CRPIX1'] \n",
    "\n",
    "            y_pix = (y_hpc/header['CDELT1']) + header['CRPIX2']\n",
    "            \n",
    "            id_team = row['id_team']\n",
    "\n",
    "            team_dict = {'SolarSoft': 'Lockheed Martin', 'SWPC': 'NOAA'}\n",
    "\n",
    "            # print(f'{x_pix}, {y_pix},{id_team}')\n",
    "\n",
    "            this_time = convert_datetime.astropytime_to_pythondatetime(row.peak_time).strftime('%H:%M')\n",
    "\n",
    "            marker_dict = {0: 'v', 1:'^', 2:'<'}\n",
    "            \n",
    "            axis.scatter(x_pix, y_pix, s = 200, color = id_team_dict[row['id_team']], label = f'{team_dict[id_team]} @ {this_time} w/ {row.goes_class}', marker = marker_dict[number])\n",
    "    \n",
    "      # Add annotation\n",
    "    wavelength = 500.0  # Example wavelength value\n",
    "    annotation = f\"{df.img_wavelength}\"\n",
    "    axis.text(0.95, 0.95, annotation, ha='right', va='top', fontsize=20, color='white', transform=axis.transAxes, bbox=dict(facecolor='black', edgecolor='none', alpha=0.8))\n",
    "    \n",
    "    axis.legend(fontsize = 15, loc = 'lower left' )\n",
    "    \n",
    "    plot_title = helio_reg_exp_module.work_dir_from_flare_candidate_input_string(df.working_dir)\n",
    "\n",
    "        #### image datetime\n",
    "    \n",
    "    img_datetime_str = img_dict['date_time'].strftime('%H:%M:%S')\n",
    "\n",
    "    axis.set_title(f'{plot_title} @ {img_datetime_str} UTC', fontsize = 20)\n",
    "    \n",
    "    #### image datetime\n",
    "    \n",
    "    img_datetime = img_dict['date_time']\n",
    "\n",
    "    \n",
    "    return(img_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_peaks_xrs_linear_combo(axis, alexis_df):\n",
    "    \n",
    "#     gridsearch_tuple = alexis_df.iloc[0].gridsearch_clusters\n",
    "\n",
    "#     print(alexis_df.columns)\n",
    "    \n",
    "    for _, row in alexis_df.iterrows():\n",
    "        \n",
    "        xrs_times, xrs_values = row['XRS_peaks_date_time'], row['XRS_peaks_value']\n",
    "        \n",
    "        lin_comb_time, lin_comb_values = row['linear_combo_peaks_date_time'],row['linear_combo_peaks_value']\n",
    "        \n",
    "        axis.scatter(xrs_times, xrs_values, color = 'maroon', alpha = 0.5, s = 300, marker = 'D')\n",
    "        \n",
    "        axis.scatter(lin_comb_time, lin_comb_values, color = 'black', alpha = 0.5, s = 300, marker = 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xrs_lin_combo(axis, df):\n",
    "    \n",
    "    df = df.iloc[0]\n",
    "    \n",
    "    datetimes = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "\n",
    "    xrs_data = df.xray_data\n",
    "\n",
    "    linear_combo_data = df.linear_combo_fit\n",
    "    \n",
    "    axis.plot(datetimes, xrs_data, color = 'maroon', alpha = 0.5, linewidth = 10, label = f'{df.xrs_telescope}-{df.xrs_wavelength}')\n",
    "    \n",
    "    axis.plot(datetimes, linear_combo_data, color = 'black', alpha = 0.5, linewidth = 10, label = 'Predicted Vector')\n",
    "    \n",
    "    axis.legend(fontsize = 10)\n",
    "    \n",
    "    def round_the_metric(metric_value):\n",
    "        return(np.around(metric_value,3))\n",
    "    \n",
    "    axis.set_title(f'RMSE:{round_the_metric(df.RMSE)}, MSE:{round_the_metric(df.MSE)}, Etot {round_the_metric(df.E_tot)}, Vfit {round_the_metric(df.vector_fit)}', fontsize = 25)\n",
    "    \n",
    "    \n",
    "    timeseries = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "    ticks_to_use = pd.date_range(start = timeseries[0], end = timeseries[-1], freq = '5min' ).tolist()\n",
    "\n",
    "    labels_for_ticks = [time_to_convert.strftime('%H:%M') for time_to_convert in ticks_to_use]\n",
    "    \n",
    "    axis.set_xticks(ticks_to_use)\n",
    "    axis.set_xticklabels(labels_for_ticks, rotation = 45, fontsize = 20)\n",
    "    \n",
    "    x_lim_min, xlim_max = timeseries[0] - pd.Timedelta('5min'),timeseries[-1] + pd.Timedelta('5min')\n",
    "    \n",
    "#     axis.set_xlim(timeseries[0], timeseries[-1])\n",
    "    \n",
    "    axis.set_xlim(x_lim_min, xlim_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_combo_and_clusters(axis, df):\n",
    "    \n",
    "    df = df.iloc[0]\n",
    "    \n",
    "    cluster_color_dict = {0: 'blue', 1: 'orange', 2: 'green', 3: 'purple'}\n",
    "    \n",
    "    datetimes = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "    \n",
    "    linear_combo_data = df.linear_combo_fit\n",
    "    \n",
    "    axis.plot(datetimes, linear_combo_data, color = 'black', alpha = 0.5, linewidth = 10, label = 'Predicted Vector')\n",
    "\n",
    "    #plot each cluster timeseries\n",
    "    for tracking_number, cluster_label in enumerate(df.gridsearch_clusters):\n",
    "\n",
    "        axis.plot(datetimes, df.cluster_matrix[tracking_number, :], label = f'cluster {cluster_label}', color = cluster_color_dict[cluster_label], alpha = 0.5, linewidth = 6)\n",
    "    \n",
    "    axis.legend(fontsize = 10)\n",
    "    \n",
    "    \n",
    "    timeseries = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "    ticks_to_use = pd.date_range(start = timeseries[0], end = timeseries[-1], freq = '5min' ).tolist()\n",
    "\n",
    "    labels_for_ticks = [time_to_convert.strftime('%H:%M') for time_to_convert in ticks_to_use]\n",
    "    \n",
    "    axis.set_xticks(ticks_to_use)\n",
    "    axis.set_xticklabels(labels_for_ticks, rotation = 45, fontsize = 20)\n",
    "    \n",
    "    x_lim_min, xlim_max = timeseries[0] - pd.Timedelta('5min'),timeseries[-1] + pd.Timedelta('5min')\n",
    "    \n",
    "#     axis.set_xlim(timeseries[0], timeseries[-1])\n",
    "    \n",
    "    axis.set_xlim(x_lim_min, xlim_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peaktime_plot_known_flare_meta_solarsoft(axis, df):\n",
    "    \n",
    "    df = df.iloc[0]\n",
    "\n",
    "    flare_meta_file = f'{df.working_dir}/known_flares.pickle'\n",
    "\n",
    "    flare_meta_df = pickle.load(open(flare_meta_file, 'rb'))\n",
    "    \n",
    "    flare_meta = flare_meta_df[flare_meta_df.id_team == 'SolarSoft']\n",
    "    \n",
    "    color_dict = {'SolarSoft': 'yellow', 'SWPC': 'red'}\n",
    "    \n",
    "    y_lim_dict = {'SolarSoft': [.5,1], 'SWPC': [0,0.5]}\n",
    "    \n",
    "    flare_meta['start_time'] = [convert_datetime.astropytime_to_pythondatetime(this_time) for this_time in flare_meta.start_time]\n",
    "    flare_meta['end_time'] = [convert_datetime.astropytime_to_pythondatetime(this_time) for this_time in flare_meta.end_time]\n",
    "    flare_meta['peak_time'] = [convert_datetime.astropytime_to_pythondatetime(this_time) for this_time in flare_meta.peak_time]\n",
    "    \n",
    "    number = 1\n",
    "    for _, row in flare_meta.iterrows():\n",
    "        \n",
    "        y_lims = y_lim_dict[row['id_team']]\n",
    "        \n",
    "        known_peak_label = row['peak_time'].strftime('%H:%M:%S')\n",
    "        \n",
    "        known_goes_class = row['goes_class']\n",
    "        \n",
    "        this_label = f'{known_goes_class} @ {known_peak_label}'\n",
    "        \n",
    "        axis.axvspan( row['start_time'], row['end_time'], y_lims[0], y_lims[1], color = color_dict[row['id_team']], alpha = 0.1, label = 'Flare duration')\n",
    "        axis.axvline(row['peak_time'], ymin = y_lims[0], ymax = y_lims[1],color = color_dict[row['id_team']], linewidth = 10, alpha = 0.7,label = f'V-Line {number}: {this_label}')\n",
    "        number = number + 1\n",
    "        \n",
    "        \n",
    "    timeseries = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "    ticks_to_use = pd.date_range(start = timeseries[0], end = timeseries[-1], freq = '5min' ).tolist()\n",
    "\n",
    "    labels_for_ticks = [time_to_convert.strftime('%H:%M') for time_to_convert in ticks_to_use]\n",
    "    \n",
    "    axis.set_xticks(ticks_to_use)\n",
    "    axis.set_xticklabels(labels_for_ticks, rotation = 45, fontsize = 20)\n",
    "    \n",
    "    x_lim_min, xlim_max = timeseries[0] - pd.Timedelta('5min'),timeseries[-1] + pd.Timedelta('5min')\n",
    "    \n",
    "#     axis.set_xlim(timeseries[0], timeseries[-1])\n",
    "    \n",
    "    axis.set_xlim(x_lim_min, xlim_max)\n",
    "    \n",
    "    axis.legend()\n",
    "\n",
    "\n",
    "def peaktime_plot_known_flare_meta_swpc(axis, df):\n",
    "    \n",
    "    df = df.iloc[0]\n",
    "\n",
    "    flare_meta_file = f'{df.working_dir}/known_flares.pickle'\n",
    "\n",
    "    flare_meta_df = pickle.load(open(flare_meta_file, 'rb'))\n",
    "    \n",
    "    flare_meta = flare_meta_df[flare_meta_df.id_team == 'SWPC']\n",
    "    \n",
    "    if len(flare_meta) > 0:\n",
    "    \n",
    "        color_dict = {'SolarSoft': 'yellow', 'SWPC': 'red'}\n",
    "\n",
    "        y_lim_dict = {'SolarSoft': [0.5,1], 'SWPC': [0,.5]}\n",
    "\n",
    "        flare_meta['start_time'] = [convert_datetime.astropytime_to_pythondatetime(this_time) for this_time in flare_meta.start_time]\n",
    "        flare_meta['end_time'] = [convert_datetime.astropytime_to_pythondatetime(this_time) for this_time in flare_meta.end_time]\n",
    "        flare_meta['peak_time'] = [convert_datetime.astropytime_to_pythondatetime(this_time) for this_time in flare_meta.peak_time]\n",
    "        \n",
    "        number = 1\n",
    "        for _, row in flare_meta.iterrows():\n",
    "\n",
    "            y_lims = y_lim_dict[row['id_team']]\n",
    "            \n",
    "            known_peak_label = row['peak_time'].strftime('%H:%M:%S')\n",
    "            \n",
    "            known_goes_class = row['goes_class']\n",
    "        \n",
    "            this_label = f'{known_goes_class} @ {known_peak_label}'\n",
    "\n",
    "            axis.axvspan( row['start_time'], row['end_time'], y_lims[0], y_lims[1], color = color_dict[row['id_team']], alpha = 0.1, label = 'Flare duration')\n",
    "            axis.axvline(row['peak_time'],ymin = y_lims[0], ymax = y_lims[1], color = color_dict[row['id_team']], linewidth = 10, alpha = 0.7, label = f'V-Line {number}: {this_label}' )\n",
    "            \n",
    "            number = number + 1\n",
    "            \n",
    "        timeseries = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "        ticks_to_use = pd.date_range(start = timeseries[0], end = timeseries[-1], freq = '5min' ).tolist()\n",
    "\n",
    "        labels_for_ticks = [time_to_convert.strftime('%H:%M') for time_to_convert in ticks_to_use]\n",
    "\n",
    "        axis.set_xticks(ticks_to_use)\n",
    "        axis.set_xticklabels(labels_for_ticks, rotation = 45, fontsize = 20)\n",
    "\n",
    "        x_lim_min, xlim_max = timeseries[0] - pd.Timedelta('5min'),timeseries[-1] + pd.Timedelta('5min')\n",
    "    \n",
    "#     axis.set_xlim(timeseries[0], timeseries[-1])\n",
    "    \n",
    "        axis.set_xlim(x_lim_min, xlim_max)\n",
    "        \n",
    "        \n",
    "    # annotate space and time\n",
    "    \n",
    "    \n",
    "    \n",
    "    axis.legend()\n",
    "\n",
    "\n",
    "def plot_alexis_peaks(axis, df):\n",
    "    \n",
    "    alexis_df = df\n",
    "    \n",
    "    df = df.iloc[0]\n",
    "    \n",
    "    masked = alexis_df[alexis_df.ALEXIS_found == True]\n",
    "    \n",
    "    cluster_color_dict = {0: 'blue', 1: 'orange', 2: 'green', 3: 'purple'}\n",
    "    \n",
    "#     print(len(masked))\n",
    "    \n",
    "    if len(masked) != 0:\n",
    "    \n",
    "        for _, row in masked.iterrows():\n",
    "\n",
    "#                 alexis_time2 = row['ALEXIS_vector_peak']\n",
    "                \n",
    "                alexis_time2 = row['final_ALEXIS_peaktime']\n",
    "\n",
    "#                 print(alexis_time2)\n",
    "                \n",
    "#                 print('----')\n",
    "\n",
    "                goes_class = row['final_ALEXIS_goes_class']\n",
    "\n",
    "                cluster_label = row['final_cluster_label']\n",
    "                \n",
    "                \n",
    "    \n",
    "                alexis_time_label = alexis_time2.strftime('%H:%M:%S') \n",
    "        \n",
    "                label = f'{goes_class} @ {alexis_time_label}'\n",
    "#                 axis.axvline(alexis_time2, color = cluster_color_dict[cluster_label], linewidth = 10, alpha = 0.5)\n",
    "\n",
    "\n",
    "                axis.axvline(alexis_time2, color = cluster_color_dict[cluster_label], linewidth = 10, alpha = 0.5, label = f'{label}')\n",
    "#                 \n",
    "                \n",
    "                \n",
    "    timeseries = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in df.resampled_time_stamp])\n",
    "    ticks_to_use = pd.date_range(start = timeseries[0], end = timeseries[-1], freq = '5min' ).tolist()\n",
    "\n",
    "    labels_for_ticks = [time_to_convert.strftime('%H:%M') for time_to_convert in ticks_to_use]\n",
    "    \n",
    "#     print(ticks_to_use)\n",
    "\n",
    "    axis.set_xticks(ticks_to_use)\n",
    "    x_lim_min, xlim_max = timeseries[0] - pd.Timedelta('5min'),timeseries[-1] + pd.Timedelta('5min')\n",
    "    \n",
    "#     axis.set_xlim(timeseries[0], timeseries[-1])\n",
    "    \n",
    "    axis.set_xlim(x_lim_min, xlim_max)\n",
    "    axis.set_xticklabels(labels_for_ticks, rotation = 45, fontsize = 20)\n",
    "    axis.legend(fontsize = 10)\n",
    "    \n",
    "    return(alexis_df)\n",
    "\n",
    "\n",
    "def plot_vector_peaks(axis, alexis_df):\n",
    "    \n",
    "#     gridsearch_tuple = alexis_df.iloc[0].gridsearch_clusters\n",
    "\n",
    "    cluster_color_dict = {0: 'blue', 1: 'orange', 2: 'green', 3: 'purple'}\n",
    "    \n",
    "    for _, row in alexis_df.iterrows():\n",
    "        \n",
    "        vector_times, vector_values = row['vector_peaks_date_time'], row['vector_peaks_value']\n",
    "        \n",
    "#         lin_comb_time, lin_comb_values = row['linear_combo_peaks_date_time'],row['linear_combo_peaks_value']\n",
    "        \n",
    "        axis.scatter(vector_times, vector_values, color = cluster_color_dict[row['final_cluster_label']], alpha = 0.5, s = 300, marker = 'D')\n",
    "        \n",
    "     \n",
    "    \n",
    "def plot_peaks_xrs_linear_combo(axis, alexis_df):\n",
    "    \n",
    "#     gridsearch_tuple = alexis_df.iloc[0].gridsearch_clusters\n",
    "\n",
    "#     print(alexis_df.columns)\n",
    "    \n",
    "    for _, row in alexis_df.iterrows():\n",
    "        \n",
    "        xrs_times, xrs_values = row['XRS_peaks_date_time'], row['XRS_peaks_value']\n",
    "        \n",
    "        lin_comb_time, lin_comb_values = row['linear_combo_peaks_date_time'],row['linear_combo_peaks_value']\n",
    "        \n",
    "        axis.scatter(xrs_times, xrs_values, color = 'maroon', alpha = 0.5, s = 300, marker = 'D')\n",
    "        \n",
    "        axis.scatter(lin_comb_time, lin_comb_values, color = 'black', alpha = 0.5, s = 300, marker = 's')\n",
    "\n",
    "def plot_img_datetime(img_datetime,ax1,ax2,ax3,ax4):\n",
    "    \n",
    "    ax1.axvline(img_datetime, color = 'black', linewidth = 2)\n",
    "    ax2.axvline(img_datetime, color = 'black', linewidth = 2)\n",
    "    ax3.axvline(img_datetime, color = 'black', linewidth = 2)\n",
    "    ax4.axvline(img_datetime, color = 'black', linewidth = 2)\n",
    "    # ax5.axvline(img_datetime, color = 'black', linewidth = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xrs_real_data(axis, xrs_data, flare_df):\n",
    "    \n",
    "    \n",
    "    axis.plot(xrs_data.date_time, xrs_data.value/0.7, color = 'black', linewidth = 4)\n",
    "    \n",
    "    timeseries = np.array([convert_datetime.convert_timestamp_to_datetime(this_time) for this_time in flare_df.iloc[0].resampled_time_stamp])\n",
    "    ticks_to_use = pd.date_range(start = timeseries[0], end = timeseries[-1], freq = '5min' ).tolist()\n",
    "\n",
    "    labels_for_ticks = [time_to_convert.strftime('%H:%M') for time_to_convert in ticks_to_use]\n",
    "    \n",
    "#     print(ticks_to_use)\n",
    "\n",
    "    axis.set_xticks(ticks_to_use)\n",
    "    x_lim_min, xlim_max = timeseries[0] - pd.Timedelta('5min'),timeseries[-1] + pd.Timedelta('5min')\n",
    "    \n",
    "#     axis.set_xlim(timeseries[0], timeseries[-1])\n",
    "    \n",
    "    axis.set_xlim(x_lim_min, xlim_max)\n",
    "    axis.set_yscale('log')\n",
    "    axis.set_xticklabels(labels_for_ticks, rotation = 45, fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_for_movie(image_df, flares_df, xrs_df):\n",
    "\n",
    "    fig = plt.figure(figsize = (30,30))\n",
    "\n",
    "    #define size of main figure\n",
    "    gs0 = gridspec.GridSpec(1, 2, figure=fig)\n",
    "\n",
    "    #define subgrids\n",
    "    gs00 = gs0[0].subgridspec(4, 1, hspace = 0)\n",
    "\n",
    "\n",
    "    length_of_ax7_columns = len(flares_df.final_cluster_label.unique())\n",
    "\n",
    "    gs01 = gs0[1].subgridspec(length_of_ax7_columns+2,2 )\n",
    "\n",
    "\n",
    "    # define axis span within subgrid 0\n",
    "    ax1 = fig.add_subplot(gs00[0, 0])\n",
    "    ax2 = fig.add_subplot(gs00[1, 0])\n",
    "    ax8 = fig.add_subplot(gs00[2, 0])\n",
    "    ax3 = fig.add_subplot(gs00[3, 0])\n",
    "\n",
    "    #\n",
    "    # plot xrs and linear combo\n",
    "\n",
    "    plot_xrs_lin_combo(ax1,flares_df)\n",
    "\n",
    "    # plot linear combo and their components\n",
    "\n",
    "    plot_linear_combo_and_clusters(ax2, flares_df)\n",
    "\n",
    "    peaktime_plot_known_flare_meta_swpc(ax8, flares_df)\n",
    "\n",
    "    #plot solarsoft peaks\n",
    "    peaktime_plot_known_flare_meta_solarsoft(ax8, flares_df)\n",
    "\n",
    "    #plot alexis peaks\n",
    "    \n",
    "    plot_alexis_peaks(ax3, flares_df)\n",
    "    \n",
    "    #plot the peaks found\n",
    "    \n",
    "    plot_peaks_xrs_linear_combo(ax1, flares_df)\n",
    "    \n",
    "    plot_vector_peaks(ax2, flares_df)\n",
    "    \n",
    "    # plot_img_datetime(img_datetime, ax1,ax2,ax3,ax4,ax5)\n",
    "\n",
    "    plot_xrs_real_data(ax8, xrs_df, flares_df)\n",
    "\n",
    "\n",
    "\n",
    "    # Set y-axis ticks for ax1, ax2, and ax3\n",
    "    yticks = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    ax1.set_yticks(yticks)\n",
    "    ax2.set_yticks(yticks)\n",
    "\n",
    "    # Adjust tick size for ax1 and ax2\n",
    "    ax1.tick_params(axis='both', which='both', labelsize=15)\n",
    "    ax2.tick_params(axis='both', which='both', labelsize=15)\n",
    "\n",
    "    ax8.tick_params(axis='both', which='both', labelsize=15)\n",
    "    # Adjust ax1 and ax2 ylabel size and name\n",
    "    ax1.set_ylabel('Normalized Flux', fontsize = 20)\n",
    "    ax2.set_ylabel('Normalized Flux', fontsize = 20)\n",
    "\n",
    "    ax8.set_ylabel('Real XRS flux', fontsize = 20)\n",
    "\n",
    "    # Adjust ax3,  ax4 and ax5 ylabel size and name\n",
    "    ax3.set_ylabel('ALEXIS flare peaks', fontsize = 20)\n",
    "\n",
    "\n",
    "    # Remove y-axis ticks and labels for ax3, ax4, and ax5\n",
    "    ax3.set_yticks([])\n",
    "    ax3.set_yticklabels([])\n",
    "\n",
    "\n",
    "    # define axis span within subgrid 1\n",
    "    ax6 = fig.add_subplot(gs01[:2, :])\n",
    "\n",
    "    # ax6 pixel tick size and label\n",
    "    ax6.tick_params(axis='both', which='both', labelsize=15)\n",
    "    ax6.set_ylabel('Y Pixels', fontsize = 20)\n",
    "    ax6.set_xlabel('X Pixels', fontsize = 20)\n",
    "\n",
    "    img_datetime = plot_best_fit_image_movie(ax6, image_df, flares_df)\n",
    "\n",
    "    ax7 = []\n",
    "    # Create equal-sized subplots within ax6\n",
    "    for tracking_number, cluster_label in enumerate(flares_df.iloc[0].gridsearch_clusters):\n",
    "        ax = fig.add_subplot(gs01[tracking_number+2, :])\n",
    "        plot_img_zoom_ins_for_movie(ax, image_df, flares_df, tracking_number, cluster_label)\n",
    "        ax7.append(ax)\n",
    "\n",
    "    # Remove x-axis ticks and labels for ax1, ax2, ax3, and ax4\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_xticklabels([])\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_xticklabels([])\n",
    "\n",
    "    ax8.set_xticks([])\n",
    "    ax8.set_xticklabels([])\n",
    "\n",
    "    plot_img_datetime(img_datetime, ax1,ax2,ax3,ax8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_init_file = glob('/data/padialjr/jorge-helio/flare_candidates//ALEXIS_flares_w_harp_goes_class_and_known_flare_meta.movie_making_*.v3_initialize_summary_jpegs_for_movie.pickle')\n",
    "movie_init_file = glob('/data/padialjr/jorge-helio/flare_candidates/*/ALEXIS_flares_w_harp_goes_class_and_known_flare_meta.movie_making_*.v3_initialize_summary_jpegs_for_movie.pickle')\n",
    "\n",
    "\n",
    "def find_wd(this_file):\n",
    "    \n",
    "    return(helio_reg_exp_module.work_dir_from_flare_candidate_input_string(this_file))\n",
    "\n",
    "wd_list = pd.DataFrame([{'wd': find_wd(this_file)} for this_file in movie_init_file]).wd.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19, 14, 137, 149, 186, 205,279, 281, 385\n",
    "\n",
    "paper_plot_dict = {'flarecandidate_C8.2_at_2012-07-01T15_45_00_47.working':'summary_plot_explain.jpg', \n",
    "                   'flarecandidate_C9.6_at_2011-09-23T21_42_10_37.working':'start_stop_wrong.jpg', \n",
    "                   'flarecandidate_C2.3_at_2013-05-01T18_52_30_32.working':'flare_duration_ex_2.jpg', \n",
    "                   'flarecandidate_C7.0_at_2011-12-14T19_45_40_67.working':'hidden_signals.jpg', \n",
    "                   'flarecandidate_C3.4_at_2014-01-05T18_17_20_29.working':'hidden_signals_2.jpg',\n",
    "                   'flarecandidate_M2.8_at_2011-09-24T18_13_30_37.working':'euv_to_def_mag.jpg',\n",
    "                   'flarecandidate_C2.7_at_2014-11-23T18_08_40_36.working':'euv_to_def_mag_2.jpg',\n",
    "                   'flarecandidate_C1.9_at_2011-09-15T03_08_20_37.working':'background_ex.jpg',\n",
    "                   'flarecandidate_C2.3_at_2015-10-27T15_49_40_55.working':'background_ex_2.jpg',\n",
    "                   'flarecandidate_C1.8_at_2013-04-28T15_44_00_10.working':'same_ar_2_flares.jpg',\n",
    "                   'flarecandidate_C2.8_at_2014-10-20T02_01_40_10.working':'same_ar_2_flares_2.jpg',\n",
    "                   'flarecandidate_C1.9_at_2014-07-06T20_52_40_60.working':'symp_ex.jpg', \n",
    "                   'flarecandidate_C1.2_at_2014-03-20T16_06_00_21.working':'sym_ex_2.jpg',\n",
    "                   'flarecandidate_C1.9_at_2014-02-06T08_01_16_19.working':'symp_ex_3.jpg',\n",
    "                   'flarecandidate_C1.1_at_2011-11-29T01_16_00_16.working':'3_regions.jpg'}\n",
    "\n",
    "for infile_dict_key in paper_plot_dict:\n",
    "    \n",
    "#     outfile_file = \n",
    "\n",
    "    outfile_name = f'{dataconfig.DATA_DIR_PRODUCTS}/alexis_paper_report_examples/{paper_plot_dict[infile_dict_key]}'\n",
    "#     print(outfile_name)\n",
    "    random_num = 1\n",
    "\n",
    "    infile = infile_dict_key\n",
    "    \n",
    "    glob_file = glob(f'/data/padialjr/jorge-helio/flare_candidates/{infile}/ALEXIS_flares_w_harp_goes_class_and_known_flare_meta.movie_making_*.v3_initialize_summary_jpegs_for_movie.pickle')\n",
    "#     print(glob_file[3])\n",
    "    file = glob_file[random_num]\n",
    "    \n",
    "    input_dict = pickle.load(open(file, 'rb'))\n",
    "\n",
    "    image_df = pd.DataFrame([input_dict])\n",
    "\n",
    "    xrs_df = pickle.load(open(image_df.iloc[0].real_xrs_data_path, 'rb'))\n",
    "\n",
    "    flares_df = pickle.load(open(image_df.iloc[0].catalog_file, 'rb'))\n",
    "\n",
    "    plot_results_for_movie(image_df, flares_df, xrs_df)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(outfile_name, dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
